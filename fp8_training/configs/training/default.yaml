training:
  # Basic training loop control
  num_epochs: 3
  max_steps: 20000  # -1 means use epochs

  # Per-device batch size and accumulation (effective batch per device = per_device_batch_size * grad_accum)
  per_device_batch_size: 8
  gradient_accumulation_steps: 40

  # Optimizer / LR schedule
  learning_rate: 1e-3
  weight_decay: 0.01
  betas: [0.9, 0.999]
  warmup_steps: 2000

  # Checkpointing / logging
  logging_steps: 100
  eval_steps: 1000
  # save_total_limit: 3

  # Misc
  seed: 42
  output_dir: ./fp8_gpt2_output
  val_split: 0.2
