# GPT2 medium preset (gpt2-m)
model:
  name: gpt2-m
  model_name_or_path: gpt2-medium
  tokenizer_name_or_path: gpt2-medium
  revision: ""

  # architecture / config hints
  max_position_embeddings: 1024
  vocab_size: 50257
  hidden_size: 1024
  num_hidden_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  layer_norm_epsilon: 1e-5
  resid_pdrop: 0.1
  embd_pdrop: 0.1
  attn_pdrop: 0.1
