# GPT2 small preset (gpt2-s)
model:
  # logical name used by the training entrypoint
  name: gpt2-s
  # Hugging Face model id or local path to load pretrained weights
  model_name_or_path: gpt2
  tokenizer_name_or_path: gpt2

  # architecture / config hints (kept for clarity; real values come from HF configs)
  vocab_size: 50257
  max_position_embeddings: 1024
  hidden_size: 768
  num_hidden_layers: 12
  num_hidden_layers: 12
  n_inner: null
  activation_function: gelu_new
  layer_norm_epsilon: 1e-5
  resid_pdrop: 0.1
  embd_pdrop: 0.1
  attn_pdrop: 0.1
